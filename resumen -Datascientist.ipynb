{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69dc16cf-cc23-4b96-9d10-f2fdb49d4e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Análisis exploratorio\n",
    "    - Descripción de datos: maximos, minimos, medias, mediana (escalado, el valor del index medio), moda (mayor frecuencia), percentiles, distribuciónes\n",
    "    - Qty de nulos,duplicados, unique values\n",
    "    - Distribución de la variable a predecir (filterr permite identificar a que distribución se asemeja. abajo teoria de distribuciones)\n",
    "    - Variables cuantitativas con valores concentrados (conviene pasarlo a cuantiativo)\n",
    "    - Identificación de outliers (boxplot por ejemplo sirve, percentiles,etc.)\n",
    "    - Correlación de variables (https://towardsdatascience.com/an-interactive-guide-to-hypothesis-testing-in-python-979f4d62d85):\n",
    "        - Vcategorica vs V categorica: chi cuadrado test. sns.snsbarplot\n",
    "        - Vnumerica vs Vcategorica:  T Testn(menor a 2 grupos) y ANOVA test (mayor a 2 grupos)\n",
    "        -Vnumerica vs Vnumerica: Coeficiente pearson. -va de –1 (relación negativa) y +1(relación positiva. cercano a 0 no hay correlación.Mide solo la relación con una línea recta. Para visualizarlo: scatter plot. Limpiar outliers. Analiza covarianza vs varianzas\n",
    "    - Gráficos (según sean: Cuanti, Cuali o time series): histogramas, barplot, line plot, boxplot, violin plot,heatmap, scatters. Evolución en el tiempo\n",
    "    - Aprendizaje desbalanceado? -30% de una clasificación \n",
    "        - Ajuste de Parámetros del modelo: l parámetro class_weight= “balanced”. redes neurnales ajustando loss\n",
    "        - Muestras artificiales: con RandomOverSampler\n",
    "        - Modificar el Dataset: con  NearMiss\n",
    "        - Balanced Ensemble Methods: Clasificador de Ensamble que utiliza Bagging\n",
    "          https://www.aprendemachinelearning.com/clasificacion-con-datos-desbalanceados/\n",
    "\n",
    "###Tratamiento de datos\n",
    "    - feature engineering (generación de nuevas features o selección)\n",
    "        - Feature importance (Existe tambien LIME, que para cada caso particular, indica cual vue el feature que más ponderó para clasificar/regresionar)\n",
    "        - Reducción de dimensionalidad (siempre conviene lo minimo e indispensable. A veces como un embedding): reducen los features entendiendo que no se pierde información y disminuyendo el ruido y la posibilidad de overfitting\n",
    "                - k-means\n",
    "                - algoritmos jerárquicos (van a su nivel superior)\n",
    "                - Principal Component Analysis (PCA)\n",
    "                - T-distributed Stochastic Neighbor Embedding (t-SNE)\n",
    "                - SVD \n",
    "        - Seleccion de features según correlaciones\n",
    "        - Generación de nuevos features\n",
    "    - Como trabajar los nulos, duplicados y outliers\n",
    "    - Categorical features to numerical vector format (pd.get_dummies/OneHotEncoder/LabelEncoder). Category encoder es la libreria que conviene usar! onehotencoder, no dummy.\n",
    "    - Standarizar si tienen tamaños grandes y variados\n",
    "    - ajustar formatos (castear)\n",
    "    - data leakage(fuga): revisar que no haya features que no se esperaría que estuviera disponible en el momento de la predicción\n",
    "\n",
    "\n",
    "### optimizador de parametros:\n",
    "    - Grid Search\n",
    "    - Random Search\n",
    "    - basados en gradiente\n",
    "\n",
    "### Modelos: Predicción,Asociación, Clasificación, Clustering, patrones\n",
    "    - Libreria tpot es una sirve para elegir el mejor modelo, con los parametros relativamente bueno.\n",
    "    - Classification Models -Sup- (devuelve valor discrto, categoría): Logistic regresion (solo binaria), KNN,Naive bayes,SVM, Decision tree(xboost entra aca), random forest\n",
    "    - Regression Models -Sup- (devuelve valor continuo): linear regression, lasso Regresion, Ridge regression, SVM  regressor, decision tree regressor,  etc.\n",
    "    - Clustering -Unsup-(agrupa elemenos por densidad (grupos x concentración), distribución (grupos por concentración. hay que conocer la distribución de los datos), centroides (por cercanía a centroides aleatoreos. itera sobre todos los puntos) o jerarquia): \n",
    "        k-means,\n",
    "        K means – Simple,\n",
    "        K means++,K medoids, \n",
    "        Agglomerative clustering,\n",
    "        DBSCAN. Si descarta puntos\n",
    "        Afinity propagation. No descarta ningun valor pero se ubica en lugaresde alta densidad. devuelve centros de la muestra. Es en formas esfericas\n",
    "        BIRCH\n",
    "        Algoritmo de agrupamiento por Propagación de Afinidad (make_classification)\n",
    "    - Dimensionality Reduction -Unsup- (Los uso en tratamiento de datos arriba)\n",
    "    - Deep Learning -Reinf- etc.                                                                                          \n",
    "    - Time series: ARIMA,ARCH,GARCH, Facebook Prophet\n",
    "    - Clasificación con Naive Bayes:Considera que cada una de las features contribuye de manera independiente a la probabilidad predictiva.\n",
    "        Cada variable independiente aumenta o disminuye la probabilidad a que se predija que es un hombre (si tengo altura, peso y talle)\n",
    "    - Ensambles: \n",
    "    - Otros: Cadena de markov\n",
    "    - Redes\n",
    "        - learning_rate, relacionado a optimazer\n",
    "        - loss: muy parecido al error que voy a medir \n",
    "        - optimzer (velocidad de corrección)\n",
    "        - epochs: qty de iteraciones sobre el train\n",
    "        - batch: qty de muestras con la que se updatea el error del modelo en el entrenamiento. Mientras más grande el batch, más lento entrena por acumular datos temporales, pero se vuelve más preciso porque ajusta los parametros en base a muestras grandes. Si entrenamos con batch chicos, va a ser mas rapido, pero con menor precisión porque cada ajuste se hace según una muestra chica\n",
    "        - accuracy: aciertos sobre intentos\n",
    "\n",
    "### Entrenamiento\n",
    "\n",
    "### Resultados & Evaluación & Métricas\n",
    "    - underfitting (entrenamiento pobre. tengo high bias y low variance) y overfitting (me pase de mambo. tengo low bias pero high variance). Cuando el error de la validación es minimo, es el punto optimo\n",
    "        - precisión/low variance (dispersión): En el entrenamiento, todas las muestras dan al mismo punto. \n",
    "        - exactitud/low bias (posición media): En el entrenamiento, el valor medio coincide con el verdadero valor de la magnitud medida\n",
    "        - La validación tiene alta precisión/low variance y baja exactitud/low bias: Overfitting. Muchos epochs, valores duplicados en entrenamiento\n",
    "        - La validación tiene alta exactitud/low bias pero baja precisión/low variance: Underfitting. X ruido de entrenamiento, se entreno poco, muestra chica, \n",
    "    - metricas: MAE,MSE,RMSE,R2,R2 ajustado \n",
    "    - Confusion matrix: calcular Precision, Recall, F1, Accuracy \n",
    "    - Curvas  ROC (falsos negativos, etc.)\n",
    "    - validación cruzada (cross_val_score): busca garantizar que los resultados son independientes de la partición train/test. Lo que hace: Particiona de forma diferente en train y test, digamos 6 veces y despues compara los resultados\n",
    "                                                                                         \n",
    "                                                                                         \n",
    "\n",
    "Distribuciones:\n",
    "    CONTINUAS\n",
    "        - Uniforme\n",
    "        - Normal\n",
    "        - Gamma\n",
    "        - chi cuadrado (caso especial de gamma)\n",
    "        - Pareto\n",
    "        - t student\n",
    "                                                                                         \n",
    "    DISCRETAS\n",
    "        - Uniforme (1 dado)\n",
    "        - bernulli (binario, con una proba)\n",
    "        - Geometrica.La probabilidad de tener un exito en tantos intentos\n",
    "        - Pascal/Distribución binomial negativa. Cantidad de dias transcurridos 4 fallas de maquina\n",
    "        - Poisson (Se especializa en la probabilidad de ocurrencia de sucesos con probabilidades muy pequeñas en el tiempo. Ej: falla en maquinas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47edfe59-c8d1-4569-a0c7-749cb8ec73cc",
   "metadata": {},
   "source": [
    "- SUPERVISADO\n",
    "    - REGRESION Devuelve un valor como predicción (time series es un tipo, donde las variables dependientes son temporales)\n",
    "         Regresión lineal (Ride o Lasso), Regresión polinomeal, SVM (Support vector machine) - regresión , decisiontreeregressor,randomforestregressor, Redes neuronales.\n",
    "         algunos para time series: ForecasterAutoreg,ForecasterAutoregCustom,ForecasterAutoregMultiOutput,LSTM,MonteCarlo\n",
    "\n",
    "    - CLASIFICACIÓN Devuelve una categoria \n",
    "        Logistic regression, Naive Baye, K-NN (k-nearest)/Vecinos más cercanos, SVM (Support vector machine),  Descision trees - clasificación, random forest - clasificación,Neuronal Networks..\n",
    "        \n",
    "        \n",
    "- NO SUPERVISADO\n",
    "    - CLUSTERING: Exclusivo, Aglomerativo, Solapamiento, Probabilístico \n",
    "Divide data by similar features. Usa para: tipo de cliente, imagenes, points in maps (Clients in high or low)\n",
    "        . Allgorithms: K-means, Agglomeratime, Mean-shiff, fuzzy c-means, \n",
    "    - PATTERN SEARCH/ Search relations/ ASOCIACION. used in goods bought together, place products in shelves (If some buy a laptom, seguramente wants a mouse)\n",
    "         .allgorithms:Euclat, Apriori, FP-growth\n",
    "\n",
    "   -DIMENSION REDUCTION. used for Recomend systems, risk managments, text. Create grupos by more relevant features (Netflix, identificar principales palabras de texto y los agrupa en : textos educativos, entretenimineto, etc.)\n",
    "        . Algorithsms: PCA, SVD, LDA\n",
    "\n",
    "\n",
    "- REFUERZO.There is no data, but yes an enviroment (Ej: playing chess with the roules, games, snake, self-driving cars, vacuums robots\n",
    "        .Allgorithsms: Genetic Algorithm, SARSA, A3C, Qlearning, Deep Q-network\n",
    "         Redes neuronales \n",
    "         \n",
    "         \n",
    "ENSAMBLE (pipline): Use as many allgorithms as possible and choose the average solution. \n",
    "        .Usually in decision trees in forest, or using many algorithms (Any)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4711a7fd-31aa-47e7-a74a-eed6707018c7",
   "metadata": {},
   "source": [
    "    NLP\n",
    "        Clasificar texto: Beto (Auto-Encoder)\n",
    "        predicción de palabras: Auto-Decoder \n",
    "        Resumir texto: Encoder-Decoder \n",
    "        identificar categorías: LDA (Latent Dirichlet Allocation) unsupervised\n",
    "        Traducción:Encoder-Decoder\n",
    "        chatbot: Encoder-Decoder\n",
    "        Reconocimiento de voz: ? investigar wev2vec2?\n",
    "\n",
    "################################# NLP especialidad\n",
    "\n",
    "#########sirve para: chatbots, resumenes, correción, predicción de palabra, generación de texto,clasificación de texto,\n",
    "\n",
    "########Arquitecturas de redes neurnales:\n",
    "RNN - recurrent neuronal networks\n",
    "LSTM - Long short term memory\n",
    "TRANSFORMERS. algoritmos pretrain a ser fine-tune: BERT (encoder)/BETO(encoder)/GPT3(decoder)\n",
    "\n",
    "\n",
    "#################Procesos NLP:\n",
    "Limpieza (stopwrods, lower, simbols,special characters etc.)\n",
    "Tokenización (cada palabra pasa a ser una unidad. la palabra es la unidad minima),\n",
    "Lemmatización/stemmatización (busca la relación entre las palabras y busca su raiz madre. ya sea por sufijos/prefijos o mismo verbo por ejemplo. Correr,corren, corri)\n",
    "Sequencing (cada palabra pasa a ser un numero)\n",
    "Padding,(para el armado de frases, define la dimensión de un vector, tal que si hay menos palabras, pone ceros)\n",
    "POS- tagging part of speach (estructura en verbos, sustantivos, etc.),\n",
    "Chunking - Process of extracting phrases (chunks) from unstructured text. Identifica frases, que las palabras por si solas significan otra cosa! ejemplo, check out, run out,etc.\n",
    "NER (name entity recognition. reconoce persona, entidad, empresa, localidad)\n",
    "embedding (cada palabra pasa a ser un vector de X dimensión según la cantidad de palabras. Una red neurnal que tiene de input la sequencia de tokens, y devuelve un vector de 6 dimensiones por ejemplo), \n",
    "\n",
    "positional encoder (solo para transformers)\n",
    "\n",
    "Librerias NLP:\n",
    "    para redes: tensorflow, keras\n",
    "    ejercicios de NLP: NLTK,pytroch(facebook), transformers, SpaCy, TextBlob, nlp (Libreria de huggingface de Datasets)\n",
    "    \n",
    " ###### Comunidades NLP\n",
    "dotcsv\n",
    "codificando bits\n",
    "towardsdatacience\n",
    "kaggle\n",
    "huggingface\n",
    "medium\n",
    "nlp en es (huggingface)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f235f49-cc30-4e1a-994b-a4efe8b74651",
   "metadata": {},
   "source": [
    "## Exploración"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "aef86ca2-0c5a-4455-a8b1-893d13471701",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  2,  4,  6,  8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32,\n",
       "       34, 36, 38])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "684544d9-841f-4137-be5d-6ff9766af974",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df=pd.DataFrame({'feature1':range(0,20),'feature2':np.arange(0, 40, 2),'feature3':np.linspace(0, 2, 20),'feature4':np.zeros(20),'feature5':None,'label':['a','b','b','b','a','c','b','b','b','c','b','c','a','b','d','d','d','a','c','d']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "58440ab9-babc-45b1-ae91-7d8617b5ca9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature1</th>\n",
       "      <th>feature2</th>\n",
       "      <th>feature3</th>\n",
       "      <th>feature4</th>\n",
       "      <th>feature5</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0.210526</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>0.315789</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>0.421053</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>0.526316</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "      <td>c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>12</td>\n",
       "      <td>0.631579</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>14</td>\n",
       "      <td>0.736842</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>16</td>\n",
       "      <td>0.842105</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>18</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "      <td>c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>20</td>\n",
       "      <td>1.052632</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>22</td>\n",
       "      <td>1.157895</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "      <td>c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>24</td>\n",
       "      <td>1.263158</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>26</td>\n",
       "      <td>1.368421</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>28</td>\n",
       "      <td>1.473684</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "      <td>d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>30</td>\n",
       "      <td>1.578947</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "      <td>d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>32</td>\n",
       "      <td>1.684211</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "      <td>d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>34</td>\n",
       "      <td>1.789474</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>36</td>\n",
       "      <td>1.894737</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "      <td>c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>38</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "      <td>d</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    feature1  feature2  feature3  feature4 feature5 label\n",
       "0          0         0  0.000000       0.0     None     a\n",
       "1          1         2  0.105263       0.0     None     b\n",
       "2          2         4  0.210526       0.0     None     b\n",
       "3          3         6  0.315789       0.0     None     b\n",
       "4          4         8  0.421053       0.0     None     a\n",
       "5          5        10  0.526316       0.0     None     c\n",
       "6          6        12  0.631579       0.0     None     b\n",
       "7          7        14  0.736842       0.0     None     b\n",
       "8          8        16  0.842105       0.0     None     b\n",
       "9          9        18  0.947368       0.0     None     c\n",
       "10        10        20  1.052632       0.0     None     b\n",
       "11        11        22  1.157895       0.0     None     c\n",
       "12        12        24  1.263158       0.0     None     a\n",
       "13        13        26  1.368421       0.0     None     b\n",
       "14        14        28  1.473684       0.0     None     d\n",
       "15        15        30  1.578947       0.0     None     d\n",
       "16        16        32  1.684211       0.0     None     d\n",
       "17        17        34  1.789474       0.0     None     a\n",
       "18        18        36  1.894737       0.0     None     c\n",
       "19        19        38  2.000000       0.0     None     d"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "5d37eee0-ed64-4721-8314-15488e0a7271",
   "metadata": {},
   "outputs": [],
   "source": [
    "#transforma a numeros los labels\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "le = LabelEncoder()\n",
    "df['10'] = le.fit_transform(df['feature1'])                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "85cee3b7-aa1e-4030-a1e7-15b74cb5b7a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df['10'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4c82d7-1ed8-4e79-a49f-fed40db57329",
   "metadata": {},
   "outputs": [],
   "source": [
    "a tener en cuenta:\n",
    "    \n",
    "df.iloc[68,1:-12]\n",
    "df.loc[]\n",
    "'# nulls in train: {variable1}'.format(df.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92f7604-3146-4c8f-b974-336b860d364b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c5d3f8-fc91-44b2-8d51-df27c1744590",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()\n",
    "# df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d92d89a3-b08e-4593-9ceb-ee72e833b25f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20 entries, 0 to 19\n",
      "Data columns (total 6 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   feature1  20 non-null     int64  \n",
      " 1   feature2  20 non-null     float64\n",
      " 2   feature3  20 non-null     float64\n",
      " 3   feature4  20 non-null     float64\n",
      " 4   feature5  0 non-null      object \n",
      " 5   label     20 non-null     object \n",
      "dtypes: float64(3), int64(1), object(2)\n",
      "memory usage: 1.1+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature1</th>\n",
       "      <th>feature2</th>\n",
       "      <th>feature3</th>\n",
       "      <th>feature4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>20.00000</td>\n",
       "      <td>20.00000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>9.50000</td>\n",
       "      <td>2.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>5.91608</td>\n",
       "      <td>1.24549</td>\n",
       "      <td>0.622745</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>4.75000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>9.50000</td>\n",
       "      <td>2.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>14.25000</td>\n",
       "      <td>3.00000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>19.00000</td>\n",
       "      <td>4.00000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       feature1  feature2   feature3  feature4\n",
       "count  20.00000  20.00000  20.000000      20.0\n",
       "mean    9.50000   2.00000   1.000000       0.0\n",
       "std     5.91608   1.24549   0.622745       0.0\n",
       "min     0.00000   0.00000   0.000000       0.0\n",
       "25%     4.75000   1.00000   0.500000       0.0\n",
       "50%     9.50000   2.00000   1.000000       0.0\n",
       "75%    14.25000   3.00000   1.500000       0.0\n",
       "max    19.00000   4.00000   2.000000       0.0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8e6d6a01-9189-4b82-ba83-dee7869cd974",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "variable1    20\n",
       "variable2    20\n",
       "dtype: int64"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#qty valores unicos por columna\n",
    "df.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a96bfaf6-0d2e-44ab-bd45-f54539cc43cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     1\n",
       "1     1\n",
       "18    1\n",
       "17    1\n",
       "16    1\n",
       "15    1\n",
       "14    1\n",
       "13    1\n",
       "12    1\n",
       "11    1\n",
       "10    1\n",
       "9     1\n",
       "8     1\n",
       "7     1\n",
       "6     1\n",
       "5     1\n",
       "4     1\n",
       "3     1\n",
       "2     1\n",
       "19    1\n",
       "Name: variable1, dtype: int64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['variable1'].value_counts()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "67543416-746f-4323-a07c-df9e179784a8",
   "metadata": {},
   "source": [
    "df['variable1'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c181c8a-819c-4d57-93be-dc4b1e4420d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4ebc1cc2-621a-43bb-ad43-16d218c038ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "variable1    0\n",
       "variable2    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#qty valores nulos por columna\n",
    "df.isna().sum().sort_values()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b31ead00-6e49-4eff-9518-dd7fd55fda27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#qty valores duplicados por columna\n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11faedd-23c2-45a7-9a38-c157488986de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tablas pivots o group by \n",
    "df[['label','feature1']].groupby('label').count().sort_values('feature1',ascending=False)\n",
    "\n",
    "df.pivot_table(index=['Year', 'Semana', 'Site'], values=['GB', 'Fee'], aggfunc=np.sum).reset_index().sort_values(by='GB', ascending=True) # si agrego .reset_index me vuelve a la base plana!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372d63c8-3d31-4fdf-b957-6e22e96f9232",
   "metadata": {},
   "outputs": [],
   "source": [
    "#A que distribución se asemeja una variable?\n",
    "\n",
    "distribuciones = ['cauchy', 'chi2', 'expon',  'exponpow', 'gamma',\n",
    "                  'norm', 'powerlaw', 'beta', 'logistic']\n",
    "\n",
    "fitter = Fitter(df['varialbe1'], distributions=distribuciones)\n",
    "fitter.fit()\n",
    "fitter.summary(Nbest=10, plot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e3d88c-0833-4600-b40d-f08ff455537c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Todas las modificaciones se realizan sobre el ultimo grafico\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_agg import FigureCanvasAgg\n",
    "from matplotlib import style\n",
    "from matplotlib.figure import Figure\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "%matplotlib notebook #Hace que sea interactivo\n",
    "# toda la clasificacion de graficos! en https://www.data-to-viz.com/ y con el codigo\n",
    "\n",
    "plt.figure(figsize=[16,4]) # create a new figure. CREATE. If note, you will modify last figure\n",
    "plt.clf() # Start over\n",
    "\n",
    "TIPOS DE GRAFICOS\n",
    "plt.subplots(5,2,8, sharex=True, sharey=True) #(2 columnas), (3 filas), s mi 8vo graph). lo que plotee ahora estara en esta posiciónplt.plot(3, 2, '.') #puntos \n",
    "\n",
    "# muchos puntos\n",
    "plt.scatter(Array,Array, s=10, c='red', label='Tall students') \n",
    "\n",
    "plt.stackplot([2,3,4],[6,3,8],[9,10,4],colors=['g','m'])\n",
    "plt.plot(Array1, '-o',Array2, '-o') #Linea\n",
    "plt.plot([22,44,55], '--r') #Linea punteada\n",
    "\n",
    "plt.bar([1,3,5,7,9],[5,13,8,2,9], width = 0.3) #barras\n",
    "\n",
    "#Histograma\n",
    "plt.hist(df['gamma'], bins=100) #Histograma\n",
    "\n",
    "plt.boxplot([ df['normal'], df['random'], df['gamma'] ], whis='range') # Box and Whisker Plots. 'range' cause dont show outliers\n",
    "\n",
    "sns.countplot(x='label',data=df)\n",
    "\n",
    "g.set_xticklabels(g.get_xticklabels(),rotation=90);plt.hist2d(Array,Array, bins=100) #Heatmap\n",
    "\n",
    "pd.scatter_matrix(X_train, c= y_train, marker = 'o', s=40, hist_kwds={'bins':15}, figsize=(9,9), cmap=cmap) # analiza correlaciones individuales de un grupo de features\n",
    "\n",
    "sns.violinplot( x= colum, y= 'precio',data = datos,color = \"white\",ax    = axes[i])\n",
    "\n",
    "FORMATOS\n",
    "plt.xlabel('TITLE') # add a label to the x axis\n",
    "plt.ylabel('aXIS y') # add a label to the y axis\n",
    "plt.title('Axis X') # add a title\n",
    "matplotlib.rcParams.update({'font.size': 16}) #modificas tamaño de letra\n",
    "plt.pie\n",
    "plt.legend(loc=4, frameon=False, title='Legend') # add a legend (uses the labels from plt.scatter) loc=(1,0) tambien sirve!\n",
    "plt.subplots_adjust(bottom=0.25) # adjust the subplot so the text doesn't run off the image\n",
    "plt.grid #Fondo rayado\n",
    "plt.colorbar() #Add a colorbar legend\n",
    "plt.xticks((pivot30.index), ('V1', 'V2', 'V3', 'V4', 'V5')) #para nombrar a las variables del eje X\n",
    "plt.set_xticklabels(g.get_xticklabels(),rotation=90);\n",
    "\n",
    "ESTILO\n",
    "style.use('ggplot') #por ejemplo. \n",
    "plt.style.available para ver todos los estilos\n",
    "\n",
    "EJES ???\n",
    "ax = plt.gca() # get current axes. Los ejes ahora se llaman ax\n",
    "ax.axis([0,6,1,10]) # Set axis properties [xmin, xmax, ymin, ymax]\n",
    "plt.gca().get_children()\n",
    "\n",
    "\n",
    "#Funcion lineal\n",
    "plt.subplots(figsize = (12,6))\n",
    "fig=data.set_index(\"Date\")[\"price_use\"].plot()\n",
    "fig.set(xlabel=\"\", ylabel = \"USD/MMBTU\", title=\"Peruvian Exported Gas Price\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#Plotting distribution of markers by royalties\n",
    "f, ax = plt.subplots(figsize = (12,6))\n",
    "fig=sns.violinplot(x=\"marker\", y=\"%regalia\", data=data)\n",
    "fig.set_title(\"Distribution: Royalty by Marker\")\n",
    "plt.show(\n",
    "    \n",
    "rolling(10).mean() #grafica la media movil! de los 10 ultimos valores por ejemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1909edc0-00ca-4970-be2b-4f3376293908",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pearson correlation (Analiza correlación de variables cuantitativas)\n",
    "# pearson_corr = pd.DataFrame(train_featured.corrwith(train_featured['congestion'], method='pearson'), \n",
    "#                             columns=['congestion'])\n",
    "\n",
    "#gráfico correlación de features\n",
    "import seaborn as sb\n",
    "sb.pairplot(df.dropna(), hue='label',size=4,vars=['feature1','feature2'],kind='scatter')\n",
    "\n",
    "\n",
    "# Heatmap matriz de correlaciones\n",
    "# ==============================================================================\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(4, 4))\n",
    "\n",
    "sns.heatmap(\n",
    "     df.select_dtypes(include=['float64', 'int']).corr(method='pearson'),\n",
    "    annot     = True,\n",
    "    cbar      = False,\n",
    "    annot_kws = {\"size\": 6},\n",
    "    vmin      = -1,\n",
    "    vmax      = 1,\n",
    "    center    = 0,\n",
    "    cmap      = sns.diverging_palette(20, 220, n=200),\n",
    "    square    = True,\n",
    "    ax        = ax\n",
    ")\n",
    "ax.set_xticklabels(\n",
    "    ax.get_xticklabels(),\n",
    "    rotation = 45,\n",
    "    horizontalalignment = 'right',\n",
    ")\n",
    "\n",
    "ax.tick_params(labelsize = 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303cf447-9119-49bf-8f15-ef121f699401",
   "metadata": {},
   "source": [
    "# TRATAMIENTO DE DATOS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e7962e3d-e2db-4a69-bda3-d498d7879029",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['feature1'].split(1,expand=Truee)\n",
    "df.replace({1:'uno ,dos',3:'tres,tee'})\n",
    "df = df['feature1','feature2'] #renombro columnas según el orden\n",
    "\n",
    "\n",
    "#Seleccion de columnas\n",
    "df = df[['feature1','feature2']]\n",
    "#Cambio nobmres columna\n",
    "df.columns = ['variable1','variable2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f50bc9d-a197-4ba9-bd57-3e9feb5e0bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Valores nulos\n",
    "df.dropna()\n",
    "\n",
    "#Valores duplicados\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "041b6554-29a8-489c-9bca-cf85e96f1b30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['a', 'b', 'c', 'd'], dtype=object)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tratamiento features categoricos\n",
    "\n",
    "# Category Encoders es la libreria. Dummy queda obsoleto, porque por ejemplo, si tengo test y train, y en test no hay una categoría. esto rompe con dummy, no con onehotencoder (o otras funciones de category encoder)\n",
    "\n",
    "\n",
    "#transforma a numeros los labels\n",
    "le = LabelEncoder()\n",
    "df['categ'] = le.fit_transform(df.label)\n",
    "\n",
    "\n",
    "# Los features categóricos fueron sometidos a distintos tipos de “encoding” (JamesStein, OneHotEncoder, HashingEncoder, HelmertEnconder, etc.).\n",
    "#Dummys tambien sirve\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "# # Convert the 10 bacteria names to the integers 0 .. 9\n",
    "le = LabelEncoder()\n",
    "df_train['category'] = le.fit_transform(df_train.label)\n",
    "# dummy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce32f0c3-6aed-4a76-a354-627a24119182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standalizar el input si hay valores de escalas muy diferentes\n",
    "\n",
    "#Opcion 1\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# scaler = StandardScaler().fit(df_train[features])\n",
    "\n",
    "# df_train[features] = scaler.transform(df_train[features])\n",
    "# df_test[features]= scaler.transform(df_test[features])\n",
    "\n",
    "\n",
    "\n",
    "#Otra opción para standarizar\n",
    "# A veces, para standarizar la información y que todas las features tengan misma importancia \n",
    "# (sino a veces una por la escala tiene mayor o menor peso) se hace lo siguiente. todas varian entre 0 y 1\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_R1, y_R1,\n",
    "                                                   random_state = 0)\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "# we must apply the scaling to the test set that we computed for the training set\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73809e57-ac8b-43c0-b185-dcc4397a8bbd",
   "metadata": {},
   "source": [
    "Exploración con modelos de kmeans (no supervisado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91495130",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### KMEANS\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import pairwise_distances_argmin_min\n",
    "\n",
    "df_km=df.copy()\n",
    "X_km=df_km.drop(['label'],1)\n",
    "Y_km=df_km[['label']]\n",
    "\n",
    "#Matodo1\n",
    "# from sklearn.metrics import silhouette_score\n",
    "# range_n_clusters=[2,3,4,5,6,7]\n",
    "# for n_clusters in range_n_clusters:\n",
    "#     clusterer = KMeans(n_clusters=n_clusters)\n",
    "#     preds = clusterer.fit_predict(X_km)\n",
    "#     centers = clusterer.cluster_centers_\n",
    "#     score = silhouette_score(X_km, preds)\n",
    "#     print(\"For n_clusters = {}, silhouette score is {})\".format(n_clusters, score))\n",
    "    \n",
    "#Maetodo2\n",
    "Nc = range(1, 20)\n",
    "kmeans = [KMeans(n_clusters=i) for i in Nc]\n",
    "score = [kmeans[i].fit(X_km).score(X_km) for i in range(len(kmeans))]\n",
    "plt.plot(Nc,score)\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Elbow Curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "744116a6-e8ea-44d5-84c9-054881954abe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>categ</th>\n",
       "      <th>label_kmeans</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">a</th>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">b</th>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">c</th>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d</th>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    index\n",
       "categ label_kmeans       \n",
       "a     0                 2\n",
       "      1                 1\n",
       "      2                 1\n",
       "b     0                 3\n",
       "      1                 1\n",
       "      2                 4\n",
       "c     0                 1\n",
       "      1                 1\n",
       "      2                 2\n",
       "d     1                 4"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import pairwise_distances_argmin_min\n",
    "kmeans = KMeans(n_clusters=3).fit(X_km)\n",
    "centroids = kmeans.cluster_centers_\n",
    "# print(centroids) #Me dice cuales son los puntos n_clusters que se usaran como centros\n",
    "\n",
    "#Predigo cada fila del dataset según el kmeans\n",
    "# kmeans.predict([X_train.loc[1].tolist()])\n",
    "Y_groupkm=kmeans.predict(X_km)\n",
    "score=kmeans.score(X_km)\n",
    "\n",
    "#Creo un data frame para comparar las categorias que tengo con los que creo el kmeans\n",
    "df_km['label_kmeans']=Y_groupkm\n",
    "df_km['categ']=Y_km\n",
    "df_km.reset_index(inplace=True)\n",
    "df_km[['label_kmeans','categ','index']].groupby(['categ','label_kmeans']).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b99d3e-ecdf-47bb-8df8-46dfddbeb3dd",
   "metadata": {},
   "source": [
    "Configuraciónes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847c78f6-39ba-43ff-93b1-ccbef334c161",
   "metadata": {},
   "outputs": [],
   "source": [
    "Optimizador de parametros\n",
    "\n",
    "#OPTIMIZADOR DE PARAMAMETROS DEL RANDOM FOREST. Nos va a decir, según las opciones que le demos, cual es la combinación más eficiente de:\n",
    "# n_estimators,max_depth,min_samples_leaf\n",
    "\n",
    "n_estimators=[200,350,500,700]\n",
    "max_depth=[20,30,50,70]\n",
    "min_samples_leaf=[2,5,10]\n",
    "grid_param={'n_estimators':n_estimators, 'max_depth':max_depth, 'min_samples_leaf':min_samples_leaf}\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "RFR=RandomForestRegressor(random_state=1)\n",
    "RFR_random= RandomizedSearchCV(estimator= RFR, param_distributions= grid_param, n_iter=500, \n",
    "                               cv=5, verbose=2, random_state= 42, n_jobs=-1)\n",
    "\n",
    "RFR_random.fit(X_trainb,Y_trainb)\n",
    "print(RFR_random.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb35018-2022-48ab-8440-9033a67c85a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizar, Sequenciar y Paddleing\n",
    "\n",
    "# from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "# tokenizer=Tokenizer(num_words=10000,oov_token='<UNK>') #Si la palabra no esta entre las 10.000 mas usadas, se guarda como <UNK>\n",
    "# tokenizer.fit_on_texts(tweets)\n",
    "\n",
    "# tokenizer.texts_to_sequences([tweets[0]])\n",
    "# lengh=[len(t.split(' ')) for t in tweets]\n",
    "\n",
    "# sequences=tokenizer.texts_to_sequences(tweets)\n",
    "# padded=pad_sequences(sequences,truncating='post',padding='post',maxlen=maxlen) #Rellena con 0s para que todos los tweets tengan mismo largo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "760dfce9-4bc2-4865-877b-1d9475711317",
   "metadata": {},
   "outputs": [],
   "source": [
    "#crear test,train y validation\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, X_test, Y_train, Y_test = train_test_split(df.drop(['label'],1),df['label'],test_size=0.1)\n",
    "\n",
    "#con validation\n",
    "# X_train, X_test, Y_train, Y_test = train_test_split(df.drop(['label'],1), df['label'], train_size=0.8,random_state=42)\n",
    "# X_train, X_valid, Y_train, Y_valid = train_test_split(X_train, Y_train, train_size=0.9,random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c0a366-945b-4ca0-8549-34e8447c36c1",
   "metadata": {},
   "source": [
    "Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db97002",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC #Suport vector machine (SVM)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\n",
    "# from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "# from sklearn.pipeline import Pipeline\n",
    "\n",
    "forest\n",
    "decisiontree\n",
    "Gradient Boosting\n",
    "SVM\n",
    "KNN\n",
    "\n",
    "#For reggresion\n",
    "# extratreesRegressor() #El mejorcito! se lo usa en topes por ejemplo\n",
    "#model= LinearRegression()\n",
    "# model=Lasso(alpha=2.0, max_iter = 10000)\n",
    "# model= Ridge(alpha=20.0)\n",
    "\n",
    "#For clasification binaria\n",
    "# model= LogisticRegression(C=100)\n",
    "\n",
    "\n",
    "#For clasification\n",
    "model = RandomForestClassifier(n_estimators=100,n_jobs=-1,random_state=42)\n",
    "# model = ExtraTreesClassifier(n_estimators=100,n_jobs=-1,random_state=42)\n",
    "# model = XGBClassifier(n_jobs=-1,n_estimators=1000,random_state=42)\n",
    "# model = KNeighborsClassifier(n_neighbors=1,n_jobs=-1)\n",
    "# model = MLPClassifier(random_state=1,max_iter=500,hidden_layer_sizes=[32,64,128,10],alpha=0.001) \n",
    "# model = SVC(kernel = 'linear', C=this_C) SVC(C=10) #Suport vector machine (SVM) clasification\n",
    "# model = KNeighborsClassifier(n_neighbors = 5)\n",
    "# algoritmo Gradient Boosting Regresor??\n",
    "#  Extra Random Forest?\n",
    "\n",
    "#For time series 1\n",
    "# forecaster = ForecasterAutoreg(\n",
    "#                 regressor = RandomForestRegressor(random_state=123),\n",
    "#                 lags = 6\n",
    "#              )\n",
    "\n",
    "#For time series 2\n",
    "# cat_base = CatBoostRegressor(\n",
    "#     #ignored_features=ignore_cols,\n",
    "#     cat_features=object_cols,\n",
    "#     eval_metric='MAE'\n",
    "# )\n",
    "\n",
    "\n",
    "#Armado de piplines\n",
    "# knn = KNeighborsClassifier(n_neighbors=1,n_jobs=-1)\n",
    "# model = Pipeline([\n",
    "#     ('scale', StandardScaler()),\n",
    "#     ('pca', PCA()),\n",
    "# ('lda', LinearDiscriminantAnalysis(n_components=9)),\n",
    "#     ('knn', knn)])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#############    REDES NEURONALES    #############\n",
    "\n",
    "# https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding\n",
    "\n",
    "# - loss='sparse_categorical_crossentropy',  #Se define el error. Se define el objetivo, la función del error minimo que se quiere tener (hay infinitas de funciones de error minimo!!). Ejemplo:'sparse_categorical_crossentropy' que se usa para problemas de clasificacion\n",
    "# Ejemplo, si tengo outlayers y tomo una el loss square (error al cuadrado), como estan mas lejos van a tener mayor peso y el algoritmo va a ponderarlos. en vez, si es loss absolute, no van a tener tanto peso a la hora de buscar ese error!\n",
    "\n",
    "# - optimizer='adam', #Se encarga de ajustar la velocidad, tiempos, para llegar al punto más optimo, y tener la mayor accuracy (precisión). Ojo con overfitting aca! Si tengo poco dato conviene ir rapido, si tengo mucho dato, conviene ir lento. per si va muy lento, hay riesgo de overfitting y que sea imposible llegar al punto optimo.\n",
    "# - metrics='accuracy'\n",
    "# - epoch: cantidad de veces que entreno con mismos datos\n",
    "# - bach: en cuanto divido el dataset para entrenar.\n",
    "# - activation function: softmax, sigma, etc.\n",
    "\n",
    "# Redes Neuronales\n",
    "# Se puede optar por embedding o One Hot Encoding (que cada palabra sea un vector de 0s con 1 uno en la unique word)\n",
    "# Embedding sirve para cualquier idioma! porque trabaja con tokens y sus relaciones en los inputs, no con texto\n",
    "\n",
    "# Embedding: OBJETIVO, ES QUE LA RED LSTM CONSIDERE PALABRAS SEMEJANTES, COMO TALES\n",
    "#lo que hace es generar 1 vector de 0s con solo 1 uno para cada unique word, y el output es el mismo vector, pero algunos 0s cambian de valor!! \n",
    "# En el entrenamiento, lo que se hace es que las mismas palabras tengan pesos semejantes segun contexto! Se entrena de forma supervisada, enseñandole que el output, es la frase.\n",
    "# model.add(Embedding(vocabulary, hidden_size, input_length=num_steps))\n",
    "# model.add(LSTM(hidden_size, return_sequences=True))\n",
    "\n",
    "\n",
    "# import tensorflow as tf\n",
    "# model = tf.keras.models.Sequential([\n",
    "#         tf.keras.layers.Embedding(10000,16,input_length=maxlen), #maxlen es la dimensión del input!. 16 output interno. Cada palabra va a estar representado con un vector de 16 dimensiones\n",
    "#         tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(20,return_sequences=True)), #Bidirecional significa que el contexto puede provenir de la derecha o izq del texto. Son 2 LSTM pero en los 2 ordenes!!\n",
    "#         tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(20)),\n",
    "#         tf.keras.layers.Dense(6,activation='softmax') #Output, 6 dimensiones (triste, happy, etc.)\n",
    "# ])\n",
    "# model.compile(\n",
    "#     loss='sparse_categorical_crossentropy',  #Se define el error buscado. 'sparse_categorical_crossentropy' se usa para problemas de clasificacion. CrossEntropyLoss\n",
    "#     optimizer='adam', #Se encarga de ajustar la velocidad, tiempos, para llegar al punto más optimo, y tener la mayor accuracy (precisión). Ojo con overfitting aca\n",
    "#     metrics='accuracy'\n",
    "# ) #loss y optimazer, usan el concepto de Gradient Descent (GD), que es cuan rapido quiero llegar al optimazer\n",
    "\n",
    "\n",
    "# The example below defines a Sequential MLP model that accepts eight inputs,\n",
    "# has one hidden layer with 10 nodes and then an output layer with one node \n",
    "# to predict a numerical value.\n",
    "# from tensorflow.keras import Sequential\n",
    "# from tensorflow.keras.layers import Dense\n",
    "\n",
    "#modelo con un input de 8, hidden de 10 y salida de 1 \n",
    "# model = Sequential()\n",
    "# model.add(Dense(10, input_shape=(8,)))\n",
    "# model.add(Dense(1))\n",
    "\n",
    "# #modelo con un input de 8, hidden de 100, hidden 80,hidden30 y salida de 1 \n",
    "# model = Sequential()\n",
    "# model.add(Dense(100, input_shape=(8,)))\n",
    "# model.add(Dense(80)) #Crea una capa de 80 neuronas\n",
    "# model.add(Dropout(0.2)) #en cada pasada de entrenamiento, aleatoreamente se eliminan 20% de la capa anterior, asi se evita overfitting!: Dropout has the effect of making the training process noisy, forcing nodes within a layer to probabilistically take on more or less responsibility for the inputs.\n",
    "# model.add(Dense(30))\n",
    "# model.add(LSTM(30,return_sequences=True, batch_input_shape=(6,3,1))) #30 units (los features),return_sequences=True, devuelve misma dimensión de entrada para cada feature\n",
    "# batch_input_shape=(6,3,1): significa que el batch es de 6, en cada unit entran 3 features,\n",
    "# model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# LSTM\n",
    "# model.add(LSTM(30,return_sequences=True, batch_input_shape=(6,3,1))) #30 units (los features),return_sequences=True, devuelve misma dimensión de entrada para cada feature\n",
    "# The output will have shape:\n",
    "# (batch, arbitrary_steps, units) if return_sequences=True.\n",
    "# (batch, units) if return_sequences=False.\n",
    "\n",
    "\n",
    "# activation: sigmoid,relu,softmax(se activa la de valor máximo), tangh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be4633a-329b-41e8-ad65-5801d922a1eb",
   "metadata": {},
   "source": [
    "Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "0c0dc110-14f8-41f0-a3b2-38533cd9232b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The batch size is a number of samples processed before the model is updated. \n",
    "# The number of epochs is the number of complete passes through the training dataset. repeticiones de mismo dataset\n",
    "\n",
    "h=model.fit(X_train, y_train)\n",
    "\n",
    "# h=model.fit(X_train,y_train, epochs=100, batch_size=32, verbose=2) #verbose:barrita de avance abajo\n",
    "# h=model.fit(X_train, y_train, X_valid, y_valid,batch_size=20, n_epochs=20, learning_rate=0.05,random_state=42)\n",
    "\n",
    "# h=model.fit(X_train, Y_train,validation_data=( X_valid, Y_valid),epochs=20,\n",
    "#     callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_accuracy',patience=3)]) #Si al segundo epoch el modelo no ve mejoras en accuracy, frena"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "ee9d59ec-e499-49f9-a2c4-2cdb4804c747",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'RandomForestClassifier' object has no attribute 'history'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-210-18eb85f500cd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mepochs_trained\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'loss'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'RandomForestClassifier' object has no attribute 'history'"
     ]
    }
   ],
   "source": [
    "#History of loss and accuracy of epochs\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import nlp #Libreria de huggingface de Datasets\n",
    "import random\n",
    "\n",
    "epochs_trained = len(h.history['loss'])\n",
    "plt.figure(figsize=(16, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(0, epochs_trained), h.history.get('accuracy'), label='Training')\n",
    "plt.plot(range(0, epochs_trained), h.history.get('val_accuracy'), label='Validation')\n",
    "plt.ylim([0., 1.])\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(0, epochs_trained), h.history.get('loss'), label='Training')\n",
    "plt.plot(range(0, epochs_trained), h.history.get('val_loss'), label='Validation')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "947a12bc-6612-4efe-9194-9541ae675e9c",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'KNeighborsClassifier' object has no attribute 'summary'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-186-5f15418b3570>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'KNeighborsClassifier' object has no attribute 'summary'"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81863fde-d6a0-4b8e-8766-e6721e95722f",
   "metadata": {},
   "source": [
    "Se analiza resultado & predicciones del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "3e1ffc4e-701e-4158-af2e-1a2f19c0cd29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict:\n",
    "Y_pred=model.predict(X_test)\n",
    "#Predict para NN en keras:\n",
    "# pred_aux=model.predict(X_test)\n",
    "# Y_pred=np.argmax(pred_aux, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "de725e3a-daf9-479a-bd2f-cf28188dd0b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Usuario\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:666: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=5.\n",
      "  warnings.warn((\"The least populated class in y has only %d\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores (3-fold): [0.33333333 0.33333333 0.33333333 0.66666667 0.        ]\n",
      "Mean cross-validation score (3-fold): 0.333\n"
     ]
    }
   ],
   "source": [
    "#Analiza la relación de features en una clasificación (profundizar un poco)\n",
    "X = X_train.values \n",
    "y = y_train.values \n",
    "cv_scores = cross_val_score(model, X, y)\n",
    "print('Cross-validation scores (3-fold):', cv_scores)\n",
    "print('Mean cross-validation score (3-fold): {:.3f}'.format(np.mean(cv_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "5e666dfe-6cbd-4d86-b96f-eef88b1f248e",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'KNeighborsClassifier' object has no attribute 'coef_'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-192-0f14aed22ad5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Principal Metrics for NN\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoef_\u001b[0m \u001b[1;31m# model coeff (w)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mintercept_\u001b[0m \u001b[1;31m#linear model intercept (b)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#'R-squared score (training)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#'R-squared score (test)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'KNeighborsClassifier' object has no attribute 'coef_'"
     ]
    }
   ],
   "source": [
    "SCORES A MEDIR, SUMADO CONFUSION MATRIZ\n",
    "\n",
    "ERRORES:    \n",
    "\n",
    "MAE (mean_absolute_error/error absoluto medio):El error promedio de las muestras. Es el más intuitivo y el que se usa para reportar. No el más util para trabajar\n",
    "MSE (Mean Squared Error/Error cuadrático medio): Representa el error promedio por muestra al cuadrado. Mientras tenga más outliers o valores atípicos, mas castigará a este kpi. Efecto contrario si todos los errores son muy pequeños\n",
    "RMSE (Root Mean Squared Error/Raiz Error cuadrático medio):Es la raiz de MSE. El error promedio x muestra. Como el MAE, pero castigado por outliers o errores groseros.\n",
    "    \n",
    "R2/coefficient of determination: 1-(MSE(vs valor real) / MSE(vs media)). MSE(vs media): es lo nuevo, y es el error más ingenuo y simplista, suponiendo que el modelo a predecir siempre será la media.\n",
    "    Es una comparación de MSE vs el MSE de un modelo simplista. Da una idea de cuan bien estoy vs este MSE de referencia. si es cercano a 1, esta muy bien.\n",
    "    Si R2 es negativo, conviene utilizar la media que el modelo para predecir (modelo malísimo).\n",
    "    El problema es que si agrego variables, siempre va a acercarce a 1. No considera el ruido como un problema! sino como ayuda a disminuir el error.\n",
    "R2 ajustado. Lo mismo que el R cuadrado, pero con una diferencia: El coeficiente de determinación ajustado penaliza la inclusión de variables.\n",
    "MSPE – Error de porcentaje cuadrático medio\n",
    "MAPE – Error porcentual absoluto medio\n",
    "RMSLE – Error logarítmico cuadrático medio\n",
    "\n",
    "# Metrics for regresions\n",
    "model.coef_ # model coeff (w)\n",
    "model.intercept_ #linear model intercept (b)\n",
    "model.score(X_train, y_train) #'R-squared score (training)\n",
    "model.score(X_test, y_test) #'R-squared score (test) El problema del coeficiente de determinación, y razón por el cual surge el coeficiente de determinación ajustado, radica en que no penaliza la inclusión de variables explicativas no significativas.\n",
    "\n",
    "model.mean_absolute_error(X_train, y_train) #(MAE)\n",
    "\n",
    "\n",
    "#Metrics forNN\n",
    "# _=model.evaluate(X_test,Y_test)\n",
    "# evaluate the model\n",
    "# loss, acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "# print('Test Accuracy: %.3f' % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7257c6dc-ecd4-4080-b3fa-2ea22984f675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time Series\n",
    "# la librería Skforecast dispone de la función grid_search_forecaster con la que comparar los resultados obtenidos con cada configuración del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "e37ba833-4fd7-4561-b92c-38351e2ffe7b",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'KNeighborsClassifier' object has no attribute 'feature_importances_'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-194-9e86289d08ca>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#Option 1,En gral\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpyplot\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mpyplot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_importances_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_importances_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mpyplot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'KNeighborsClassifier' object has no attribute 'feature_importances_'"
     ]
    }
   ],
   "source": [
    "#feature importance \n",
    "\n",
    "\n",
    "# Option 0\n",
    "#plot the catboost result\n",
    "plot_feature_importance(model.get_feature_importance(), X_train.columns, 'CATBOOST')\n",
    "\n",
    "#Option 1,En gral\n",
    "from matplotlib import pyplot\n",
    "pyplot.bar(range(len(model.feature_importances_)), model.feature_importances_)\n",
    "pyplot.show()\n",
    "\n",
    "df_feature_importance=pd.DataFrame(model.feature_importances_,index=df_train.columns[1:-2].tolist())\n",
    "df_feature_importance.sort_values(0,ascending=False).tail(50)\n",
    "\n",
    "#Option2 si uso Xboost, este sirve para feature importance\n",
    "# plot feature importance\n",
    "# plot_importance(model)\n",
    "# pyplot.show()\n",
    "\n",
    "#option3,Plot feature importance \n",
    "# from adspy_shared_utilities import plot_feature_importances\n",
    "# plt.figure(figsize=(10,4), dpi=80)\n",
    "# plot_feature_importances(model, iris.feature_names)\n",
    "# plt.show()\n",
    "# print('Feature importances: {}'.format(model.feature_importances_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f34dfdb-2fab-43bf-b107-fb491efba80e",
   "metadata": {},
   "source": [
    "<!-- Normalizar un texto -->\n",
    "<!-- df.primary_diagnosis.value_counts(normalize = True).round(2) -->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "4f57f20f-fec1-4851-a56c-5ea7955fec36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Opcion 1 de confusio\n",
    "# cm = confusion_matrix(Y_test, Y_pred, normalize='true')\n",
    "# plt.figure(figsize=(10, 10))\n",
    "# sp = plt.subplot(1, 1, 1)\n",
    "# ctx = sp.matshow(cm)\n",
    "# plt.xticks(list(range(0, len(df_train.label.unique()))), labels=df_train.label.unique().tolist())\n",
    "# plt.yticks(list(range(0, len(df_train.label.unique()))), labels=df_train.label.unique().tolist())\n",
    "# plt.colorbar(ctx)\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "#Opcion 2 de confusio\n",
    "cm = confusion_matrix(Y_test,Y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot() \n",
    "plt.show()\n",
    "\n",
    "\n",
    "#Teoria\n",
    "#CUANDO HAY SOLO 2 CATEGORIAS\n",
    "# TP=cm[0][0]\n",
    "# FN=cm[0][1]\n",
    "# FP=cm[1][0]\n",
    "# TN=cm[1][1]\n",
    "# Sens=TP/(TP+FN) #Sensibilidad \n",
    "# UnomSp=FP/(FP+TN) #(1-Specify)\n",
    "# Prec= TP/(TP+FP) # #Precisión\n",
    "# # Lo mas eficiente es sensibilidad=1, 1-Sepcify=0\n",
    "# # TP  FN  \n",
    "# # FP  TN\n",
    "#CUANDO HAY 3 CATEGORIAS\n",
    "# TP1=cm[0][0]\n",
    "# FN1=cm[0][1]+cm[0][2]\n",
    "# FN1bis=cm[1][2]+cm[2][1]\n",
    "# FP1=cm[1][0]+cm[2][0]\n",
    "# TN1=cm[1][1]+cm[2][2]\n",
    "\n",
    "# Sens=TP1/(TP1+FN1) #Sensibilidad \n",
    "# UnomSp=FP1/(FP1+TN1+FN1bis) #(1-Specify)\n",
    "# Prec= TP1/(TP1+FP1) # #Precisión\n",
    "\n",
    "# Lo mas eficiente es sensibilidad=1, 1-Sepcify=0\n",
    "# TP  FN  \n",
    "# FP  TN\n",
    "\n",
    "# print(f'Resultados de {Y_test[0]}:')\n",
    "# print(f'De los {TP1+FN1} comentarios test reales \"{Y_test[0]}\", el {round(Sens,2)*100}% fueron correctamente predictos como {Y_test[0]} (Sensibilidad)')\n",
    "# print(f'De los {TN1+FP1+FN1bis} comentarios test reales \"no {Y_test[0]}\", el {round(UnomSp*100)}% fueron erroneamente predictos como {Y_test[0]} (1-Specify)') \n",
    "# print(f'De los {TP1+FP1} comentarios test predictos como \"{Y_test[0]}\", el {round(Prec*100)}% fueron correctamente predictos como {Y_test[0]} (Precisión/Accuracy)') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "4ef45754-e966-4f1d-bfa7-f88570c6e3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parecido a confusio\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "# Accuracy = TP + TN / (TP + TN + FP + FN)\n",
    "# Precision = TP / (TP + FP)\n",
    "# Recall = TP / (TP + FN)  Also known as sensitivity, or True Positive Rate\n",
    "# F1 = 2 * Precision * Recall / (Precision + Recall) \n",
    "# print('Accuracy: {:.2f}'.format(accuracy_score(Y_test, Y_pred)))\n",
    "# print('Precision: {:.2f}'.format(precision_score(Y_test, Y_pred)))\n",
    "# print('Recall: {:.2f}'.format(recall_score(Y_test, Y_pred)))\n",
    "# print('F1: {:.2f}'.format(f1_score(Y_test, Y_pred)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088cfe84-6152-40a6-b5cc-a4ef3b72ac94",
   "metadata": {},
   "source": [
    "Guardar modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9126b8-df34-42d0-b60b-ab4d9afd4e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install h5py\n",
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6acb0ad-b1dd-4d36-8a72-eefba089d47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA ENGINEER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a529fc42-eedb-4096-98f2-f8d50f7c4e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA ENGINEERING\n",
    "\n",
    "Debug/Depuración: Encontrar y eliminar errores\n",
    "Bug: Error\n",
    "Castear: dar el formato adecuado\n",
    "Parsear: Analizar un código XML sintácticamente, y revisar si tiene todos los campos que consideramos obligatorios\n",
    "Anidar: Relacionar las bases de datos\n",
    "On premise: trabajar de forma local (contrario a trabajar en la nube\n",
    "\n",
    "\n",
    "Fromatos: Json, XML, yaml\n",
    "\n",
    "Frameworks:\n",
    "\tHadook (Spark, hive, impala)\n",
    "\n",
    "Herramientas de ambientes:\n",
    "\tVirtual Machine. VirtualBox, VMWare\n",
    "\tEntornos virtuales\n",
    "\tDocker\n",
    "\n",
    "Herramientas de accesos\n",
    "\tPostman\n",
    "Putty\n",
    "firewall\n",
    "\n",
    "\n",
    "Proceso ETL:\n",
    "Extracción/Ingesta: De base de datos con accesos con apis, o de bases locales\n",
    "transformación con spark sobre todo o pandas (usando un procesador/cluster de una nube y un ambiente como databriks y un orquestador como data factory)\n",
    "Carga sobre una base de datos propia. Almacenar en una nube como un data lake\n",
    "\n",
    "\n",
    "Infraestructuras/ DW en la nube: Azure, Oracle, AMazon y Google\n",
    "\n",
    "Azure\n",
    "Databricks. Notebooks\n",
    "Data lake. Repositorio\n",
    "Data factory (Airflow). Orquestador \n",
    "\n",
    "\n",
    "\n",
    "PARA TENER ACCESO A REDES DE EMPRESAS (DONDE TIENEN CIERTOS PROGRAMAS, APLICACIONES Y DATOS), SE NECESITA contar con UN IP publica QUE ESTE DENTRO DEL RANGO DE LAS IPs DE LA RED. Varios caminos:\n",
    "\n",
    "VPN. que biwares tenga una VPN tal que cuando nos conectamos, la VPN transforma la IP publica nuestra, en una IP valida para esa red en u proceso de encriptado o enmascarado (hay varios softwares que permiten esto. entre los más usados, Forticlient o OpenVPN). Proxy hace algo parecido pero no a nivel SO como la VPN, sino a nivel aplicación y sin software.\n",
    "Tener acceso con credenciales a una maquina virtual, la cual si tenga una IP dentro del rango aceptado por la red.\n",
    "Hacer una excepeción configurando el firewall, donde se solicita que las IPs nuestras, la de mi compu, tenga acceso a su red. (es lo más peliloso)\n",
    "\n",
    "Además de todo esto, hay algunas redes privadas que tienen otra capa de seguridad y necesitan un tunnel (SSH). kerberos tambien es algo de esto y los proxy.\n",
    "\n",
    "\n",
    "IP publica es la del modem a la que estoy conectado, y la que se actualiza cuando se resetea. IP privada es la que me diferencia de los demas diapositivos dentro de mi red (modem)\n",
    "\n",
    "\n",
    "Luego, si me quiero conectar a la base de datos, necesito un gestor de base de datos como dbeaver y saber:\n",
    "host: la url\n",
    "puerto: 421\n",
    "database: nombre\n",
    "tipo de base de datos: mySQL o MariaDB, SQL Server\n",
    "usuario y contraseña (Las credenciales)\n",
    "tipo de base de datos: (si es mysql, azure, redshift,etc. cada uno de estos tiene diferentes drivers, que son quienes funcionan de interpretador, propios de cada tecnología)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
